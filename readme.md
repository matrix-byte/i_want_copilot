学习率是一个相对来说很难设计的参数，设置过大，则有可能在学习过程中错过最优解，甚至导致学习发散，设置过小会导致学习时间过长，收敛困难。因此有着很多关于学习率的优化算法。常用的自适应学习率算法有AdaGrad算法、RMSProp算法、Adam算法等。AdaGrad算法独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根，具有损失较大偏导的参数相应地有一个快速下降的学习率，而具较小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。RMSProp算法修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。RMSProp使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛。
